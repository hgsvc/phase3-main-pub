{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8cb788ce-bc0d-4ab7-980c-25168ea7fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import collections as col\n",
    "import re\n",
    "import itertools as itt\n",
    "\n",
    "root_folder = pl.Path(\"/home/ebertp/work/projects/hgsvc/2024_debug_hsmdup\")\n",
    "\n",
    "def load_assembly_info(fai_file):\n",
    "\n",
    "    df = pd.read_csv(fai_file, sep=\"\\t\", header=None, usecols=[0,1], names=[\"contig\", \"length\"])\n",
    "\n",
    "    def plain_name(seqname):\n",
    "        if \"#\" in seqname:\n",
    "            plain = seqname.split(\"#\")[0]\n",
    "        elif \"_\" in seqname:\n",
    "            plain = seqname.split(\"_\")[0]\n",
    "        else:\n",
    "            plain = seqname\n",
    "        assert re.match(\"^[0-9hutglc]+$\", plain), plain\n",
    "        return plain\n",
    "\n",
    "    \n",
    "    df[\"plain\"] = df[\"contig\"].apply(plain_name)\n",
    "    seq_names = set(df[\"plain\"].values)\n",
    "    total_length = df[\"length\"].sum()\n",
    "    length_lookup = dict((row.plain, row.length) for row in df.itertuples())\n",
    "    return seq_names, total_length, length_lookup\n",
    "\n",
    "\n",
    "def summarize_assembly(filename):\n",
    "\n",
    "    sample = \"NA19240\"\n",
    "    assert sample in filename\n",
    "    if \"hap1\" in filename or \"hap2\" in filename:\n",
    "        asm_unit = filename.split(\".\")[0].split(\"-\")[-1]\n",
    "        assm_key = (sample, \"lin-phased\", asm_unit)\n",
    "    else:\n",
    "        if \"p_ctg\" in filename:\n",
    "            level = \"contig\"\n",
    "        else:\n",
    "            assert \"p_utg\" in filename\n",
    "            level = \"unitig\"\n",
    "        if \".bp.\" in filename:\n",
    "            phasing = \"gfa-partial\"\n",
    "        elif \".dip.\" in filename:\n",
    "            phasing = \"gfa-phased\"\n",
    "        else:\n",
    "            raise\n",
    "        assm_key = (sample, phasing, level)\n",
    "\n",
    "    return assm_key\n",
    "\n",
    "\n",
    "def compute_ab_similarity(contigs_a, contigs_b):\n",
    "\n",
    "\n",
    "    total_a = sum(contigs_a.values())\n",
    "    matched_a = 0\n",
    "    missed_seq = 0\n",
    "    missed_seq_names = set()\n",
    "    missed_seq_len = 0\n",
    "    for seq1, seqlen1 in contigs_a.items():\n",
    "        try:\n",
    "            seqlen2 = contigs_b[seq1]\n",
    "            matched_a += seqlen1\n",
    "        except KeyError:\n",
    "            missed_seq += 1\n",
    "            missed_seq_names.add(seq1)\n",
    "            missed_seq_len += seqlen1\n",
    "\n",
    "    missed_seq_pct = round(missed_seq / len(contigs_a) * 100, 2)\n",
    "    matched_bp_pct = round(matched_a / total_a * 100, 2)\n",
    "    missed_seq_len = round(missed_seq_len / int(1e6), 2)\n",
    "    return missed_seq_pct, matched_bp_pct, missed_seq_len\n",
    "    \n",
    "\n",
    "def check_assembly_consistency(key1, contigs1, key2, contigs2):\n",
    "\n",
    "    if len(set(contigs1.keys()).intersection(set(contigs2.keys()))) == 0:\n",
    "        return (key1, key2, 0, 0, -1), (key2, key1, 0, 0, -1)\n",
    "    else:\n",
    "        sim_12 = compute_ab_similarity(contigs1, contigs2)\n",
    "        sim_21 = compute_ab_similarity(contigs2, contigs1)\n",
    "        return (key1, key2, *sim_12), (key2, key1, *sim_21)\n",
    "\n",
    "\n",
    "def parse_rukki_paths(gaf_file, assembler=\"hifiasm\"):\n",
    "\n",
    "    haps = {\n",
    "        \"HAPLOTYPE1\": \"h1\",\n",
    "        \"HAPLOTYPE2\": \"h2\",\n",
    "        \"NA\": \"h0\"\n",
    "    }\n",
    "\n",
    "    re_utigs = re.compile(\"utg[0-9]+[lc]\")\n",
    "    if assembler == \"verkko\":\n",
    "        re_utigs = re.compile(\"utig4\\-[0-9]+\")\n",
    "\n",
    "    tig_assignment = col.defaultdict(set)\n",
    "    with open(gaf_file, \"r\") as table:\n",
    "        _ = table.readline()\n",
    "        for line in table:\n",
    "            _, path, assigned_hap = line.strip().split()\n",
    "            hap = haps[assigned_hap]\n",
    "            utigs_in_path = re.findall(re_utigs, path)\n",
    "            assert len(utigs_in_path) > 0, line.strip()\n",
    "            tig_assignment[hap] = tig_assignment[hap].union(set(utigs_in_path))\n",
    "            for utig in utigs_in_path:\n",
    "                tig_assignment[utig].add(hap)\n",
    "\n",
    "    #print(assembler)\n",
    "    # for a,b in itt.combinations(sorted(tig_assignment.keys()), 2):\n",
    "    #     ambig = tig_assignment[a].intersection(tig_assignment[b])\n",
    "    #     if len(ambig) > 0:\n",
    "    #         print(a, len(tig_assignment[a]))\n",
    "    #         print(b, len(tig_assignment[b]))\n",
    "    #         print(len(ambig))\n",
    "    \n",
    "    return tig_assignment\n",
    "\n",
    "\n",
    "def read_alignments(norm_paf, only_primary=True, lower_cap_mapq=0):\n",
    "\n",
    "    paf = pd.read_csv(norm_paf, sep=\"\\t\", header=0)\n",
    "    if only_primary:\n",
    "        paf = paf.loc[paf[\"tp_align_type\"] != 2, :].copy()\n",
    "        paf = paf.loc[paf[\"align_matching\"] > 10000, :].copy()\n",
    "    paf = paf.loc[paf[\"mapq\"] > lower_cap_mapq, :].copy()\n",
    "    return paf\n",
    "\n",
    "\n",
    "def assign_haplotype_to_contigs(paf, tig_assignment):\n",
    "\n",
    "    contig_haps = col.defaultdict(list)\n",
    "    for (unitig, contig), alns in paf.groupby([\"query_name\", \"target_name\"]):\n",
    "        utig = unitig.split(\"_\")[0]\n",
    "        hap = tig_assignment[utig]\n",
    "        assert len(hap) > 0\n",
    "        contig_haps[contig].append(\n",
    "            (\n",
    "                tuple(hap), utig, alns[\"align_matching\"].sum()\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return contig_haps\n",
    "\n",
    "\n",
    "def summarize_contig_haps(contig_haps, contigs):\n",
    "\n",
    "    records = []\n",
    "    \n",
    "    for contig, length in contigs.items():\n",
    "        record = {\n",
    "            \"seq\": contig,\n",
    "            \"seqlen\": length,\n",
    "            \"h1_bp\": 0,\n",
    "            \"h1_pct\": 0,\n",
    "            \"h2_bp\": 0,\n",
    "            \"h2_pct\": 0,\n",
    "            \"h0_bp\": 0,\n",
    "            \"h0_pct\": 0,\n",
    "            \"miss_bp\": 0,\n",
    "            \"miss_pct\": 0\n",
    "        }\n",
    "        if contig in contig_haps:\n",
    "            hap_assign = contig_haps[contig]\n",
    "            hap_count = col.Counter()\n",
    "            hap_count[\"miss_bp\"] = length\n",
    "            for (hap, utig, assigned_bp) in hap_assign:\n",
    "                if len(hap) == 2:\n",
    "                    assert \"h0\" not in hap\n",
    "                    hap_label = \"MX\"\n",
    "                else:\n",
    "                    assert len(hap) == 1, (hap, utig, assigned_bp)\n",
    "                    hap_label = hap[0]\n",
    "                hap_count[f\"{hap_label}_bp\"] += assigned_bp\n",
    "                hap_count[\"miss_bp\"] -= assigned_bp\n",
    "            for hap_label, hap_bp in hap_count.items():\n",
    "                record[hap_label] = hap_bp\n",
    "                pct = round(hap_bp / length * 100, 2)\n",
    "                record[hap_label.replace(\"_bp\", \"_pct\")] = pct\n",
    "            records.append(record)\n",
    "        else:\n",
    "            record[\"miss_bp\"] = length\n",
    "            record[\"miss_pct\"] = 100\n",
    "            records.append(record)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.sort_values([\"seqlen\", \"seq\"], inplace=True, ascending=[False, True])\n",
    "    return df\n",
    "\n",
    "\n",
    "tig_assignment = parse_rukki_paths(root_folder.joinpath(\"NA19240hifi_rukki_paths.gaf\"), \"hifiasm\")\n",
    "#tig_assignment = parse_rukki_paths(root_folder.joinpath(\"verkko\", \"NA19240_rukki_paths.gaf\"), \"verkko\")\n",
    "#raise\n",
    "\n",
    "assembly_infos = col.defaultdict(dict)\n",
    "\n",
    "for fai_file in root_folder.glob(\"**/*.fai\"):\n",
    "    assm_key = summarize_assembly(fai_file.name)\n",
    "    seq_names, total_length, length_lookup = load_assembly_info(fai_file)\n",
    "    assembly_infos[assm_key] = {\n",
    "        \"seq_names\": seq_names,\n",
    "        \"total_length\": total_length,\n",
    "        \"seq_to_length\": length_lookup\n",
    "    }\n",
    "\n",
    "records = []\n",
    "\n",
    "for a, b in itt.combinations(sorted(assembly_infos.keys()), 2):\n",
    "    a_to_b, b_to_a = check_assembly_consistency(a, assembly_infos[a][\"seq_to_length\"], b, assembly_infos[b][\"seq_to_length\"])\n",
    "    records.append(a_to_b)\n",
    "    records.append(b_to_a)\n",
    "    \n",
    "summary = pd.DataFrame.from_records(records, columns=[\"asm1\", \"asm2\", \"missed_seq_pct\", \"matched_bp_pct\", \"missed_seq_len_Mbp\"])\n",
    "\n",
    "paf_files = [\n",
    "    root_folder.joinpath(\"NA19240.utg-to-bp.p_ctg.norm-paf.tsv.gz\"),\n",
    "    root_folder.joinpath(\"NA19240.utg-to-dip.p_ctg.norm-paf.tsv.gz\")\n",
    "]\n",
    "\n",
    "aln_to_key = {\n",
    "    \"utg-to-bp\": ('NA19240', 'gfa-partial', 'contig'),\n",
    "    \"utg-to-dip\": ('NA19240', 'gfa-phased', 'contig'),\n",
    "}\n",
    "\n",
    "for paf_file in paf_files:\n",
    "    paf = read_alignments(paf_file)\n",
    "    contig_haps = assign_haplotype_to_contigs(paf, tig_assignment)\n",
    "    assm_key = aln_to_key[paf_file.name.split(\".\")[1]]\n",
    "    contig_lengths = assembly_infos[assm_key][\"seq_to_length\"]\n",
    "    hap_contig_summary = summarize_contig_haps(contig_haps, contig_lengths)\n",
    "\n",
    "    if assm_key[1] == \"gfa-phased\":\n",
    "        dip_support = read_alignments(root_folder.joinpath(\n",
    "            \"NA19240.bp-to-dip.p_ctg.norm-paf.tsv.gz\"\n",
    "        ))\n",
    "        collect_support = col.defaultdict(list)\n",
    "        for (bp_ctg, dip_ctg), alns in dip_support.groupby([\"query_name\", \"target_name\"]):\n",
    "            support = alns[\"align_matching\"].sum()\n",
    "            qry_support = round(support / alns[\"query_length\"].iloc[0] * 100, 2)\n",
    "            trg_support = round(support / alns[\"target_length\"].iloc[0] * 100, 2)\n",
    "            collect_support[dip_ctg].append(\n",
    "                f\"{bp_ctg}:Q-{qry_support}:T-{trg_support}\"\n",
    "            )\n",
    "        add_support = dict()\n",
    "        for dip_ctg, support_info in collect_support.items():\n",
    "            add_support[dip_ctg] = \"|\".join(support_info)\n",
    "        hap_contig_summary[\"bp_support\"] = hap_contig_summary[\"seq\"].apply(lambda seq: \"no-support\" if seq not in add_support else add_support[seq])\n",
    "                   \n",
    "    \n",
    "    outfile = root_folder.joinpath(\n",
    "        paf_file.name.replace(\".tsv.gz\", \".ctg-hap-summary.tsv\")\n",
    "    )\n",
    "    wg_length = hap_contig_summary[\"seqlen\"].sum()\n",
    "    with open(outfile, \"w\") as dump:\n",
    "        _ = dump.write(f\"# wg: {wg_length} bp\\n\")\n",
    "        hap_contig_summary.to_csv(dump, sep=\"\\t\", header=True, index=False)\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
