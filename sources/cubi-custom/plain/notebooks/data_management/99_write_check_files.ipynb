{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97f087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping over 19320 - not enough data\n",
      "\n",
      "Skipping over 19320 - not enough data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOFN path collision detected: HG00733_hifi_fastq.hgsvc-UW-2023.fofn exists!\n"
     ]
    }
   ],
   "source": [
    "%run \"50_match_sample_files.ipynb\"\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import collections as col\n",
    "import os\n",
    "import stat\n",
    "import hashlib as hl\n",
    "\n",
    "DRY_RUN = False\n",
    "VERBOSE = True\n",
    "DEBUG = False\n",
    "\n",
    "MANUALLY_CURATED_SOURCES = {\n",
    "    \"UW_WH\": (\"UW\", \"hgsvc\"),\n",
    "    \"JAX_PA\": (\"JAX\", \"hgsvc\"),\n",
    "    \"epi2me\": (\"epi2me\", \"epi2me\")\n",
    "}\n",
    "\n",
    "fofn_files = []\n",
    "fofn_files_out = PROJECT_BASE.joinpath(\"samples\", \"fofn_table.tsv\")\n",
    "\n",
    "def set_file_permissions(file_path):\n",
    "    \n",
    "    perm = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP | stat.S_IROTH\n",
    "    os.chmod(file_path, perm)\n",
    "    return\n",
    "\n",
    "\n",
    "def check_is_identical(file_path, new_name, new_content):\n",
    "    \n",
    "    if not file_path.is_file():\n",
    "        return False\n",
    "    \n",
    "    existing_content = []\n",
    "    with open(file_path, \"r\") as existing_file:\n",
    "        for line in existing_file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            existing_content.append(line.strip())\n",
    "        \n",
    "    is_id_content = existing_content == [str(fp) for fp in new_content]\n",
    "    is_id_name = file_path.name == new_name\n",
    "   \n",
    "    return is_id_content and is_id_name\n",
    "\n",
    "\n",
    "def compute_read_files_hash(read_files):\n",
    "    \n",
    "    casted = sorted([str(p) for p in read_files])\n",
    "    casted = \",\".join(casted)\n",
    "    file_hash = hl.sha256(casted.encode(\"utf-8\")).hexdigest()\n",
    "    return file_hash\n",
    "\n",
    "\n",
    "fofn_collisions = set()\n",
    "\n",
    "updated_sample_fofns = set()\n",
    "\n",
    "for sample_name, sample_obj in SAMPLES.items():\n",
    "    if sample_name == sample_obj.alt:\n",
    "        continue\n",
    "\n",
    "    sample_files = []\n",
    "    sample_files_main = FILES_EXIST_PER_SAMPLE[sample_name]\n",
    "    sample_files_alt = FILES_EXIST_PER_SAMPLE[sample_obj.alt]\n",
    "    \n",
    "    for sf_main in sample_files_main:\n",
    "        if sf_main in sample_files:\n",
    "            continue\n",
    "        sample_files.append(sf_main)\n",
    "    \n",
    "    for sf_alt in sample_files_alt:\n",
    "        if sf_alt in sample_files:\n",
    "            continue\n",
    "        sample_files.append(sf_alt)\n",
    "                \n",
    "    assert len(sample_files) == len(sample_files_main) + len(sample_files_alt)\n",
    "    \n",
    "    last_group = None\n",
    "    for sample_file in sample_files:\n",
    "        key = sample_file.read_type, sample_file.file_group\n",
    "        sample_obj.sample_files[key].append(sample_file.data_rel_path)\n",
    "        if sample_file.file_group != last_group:\n",
    "            sample_obj.source_dates[sample_file.file_group].append(\n",
    "                sample_file.group_date\n",
    "            )\n",
    "            sample_obj.source_dates[sample_file.file_group].append(\n",
    "                sample_file.file_date\n",
    "            )\n",
    "            last_group = sample_file.file_group\n",
    "        else:\n",
    "            sample_obj.source_dates[sample_file.file_group].append(\n",
    "                sample_file.file_date\n",
    "            )\n",
    "    \n",
    "    if sample_obj.hifi_complete:\n",
    "        for (read_type, read_group), read_files in sample_obj.sample_files.items():\n",
    "            \n",
    "            read_files_hash = compute_read_files_hash(read_files)\n",
    "                        \n",
    "            if read_type != \"hifi\":\n",
    "                continue\n",
    "            source_dates = sample_obj.source_dates[read_group]\n",
    "\n",
    "            try:\n",
    "                source_date = str(int(source_dates[0]))\n",
    "            except ValueError:\n",
    "                source_date, _ = col.Counter(source_dates).most_common(1)[0]\n",
    "\n",
    "            read_group_info = MERGED.loc[MERGED[\"file_group\"] == read_group, [\"data_source\", \"project\"]]\n",
    "            if read_group_info.empty:\n",
    "                read_group_info = set(\n",
    "                    ALL_FILES.loc[ALL_FILES[\"file_group\"] == read_group, \"curated_by\"].values\n",
    "                )\n",
    "                data_source, project = MANUALLY_CURATED_SOURCES[read_group_info]\n",
    "            else:\n",
    "                data_source = set(read_group_info[\"data_source\"].values).pop()\n",
    "                project = set(read_group_info[\"project\"].values).pop()\n",
    "                \n",
    "            if data_source == project and data_source in MANUALLY_CURATED_SOURCES:\n",
    "                data_source, project = MANUALLY_CURATED_SOURCES[data_source]\n",
    "            elif data_source == project:\n",
    "                assert data_source.startswith(\"HGSVC\") and project.startswith(\"HGSVC\"), f\"{data_source} / {project}\"\n",
    "                project = \"hgsvc\"\n",
    "                data_source = data_source[5:].upper()\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            source_path = sample_obj.get_file_group_lca_path(read_files)\n",
    "            verified_file_dir = PATH_PREFIX.local.joinpath(\n",
    "                DATA_ROOT, source_path\n",
    "            )\n",
    "            \n",
    "            # step - write verified file\n",
    "            verified_file_name = f\"{sample_obj.name}.{len(read_files)}-cells.verified\"\n",
    "            verified_file_path = verified_file_dir.joinpath(verified_file_name)\n",
    "            verified_read_files = sorted([fp.name for fp in read_files])\n",
    "            \n",
    "            if check_is_identical(verified_file_path, verified_file_name, verified_read_files):\n",
    "                if DEBUG:\n",
    "                    print(f\"=== ID - skipping update of {verified_file_path}\")\n",
    "            elif not DRY_RUN:\n",
    "                assert verified_file_dir.is_dir()\n",
    "                with open(verified_file_path, \"w\") as dump:\n",
    "                    _ = dump.write(f\"# {TODAY}\\n\")\n",
    "                    _ = dump.write(\"\\n\".join(verified_read_files) + \"\\n\")\n",
    "                set_file_permissions(verified_file_path)\n",
    "            else:\n",
    "                if VERBOSE:\n",
    "                    print(f\">>> DRY RUN - not updating: {verified_file_path}\")\n",
    "            \n",
    "            # step - write fofn file\n",
    "            fofn_name = f\"{sample_obj.name}_{read_type}_fastq.{project}-{data_source}-{source_date}.fofn\"\n",
    "            fofn_path = LOCAL_SAMPLE_ROOT.joinpath(\n",
    "                sample_obj.name, fofn_name\n",
    "            )\n",
    "            \n",
    "            # this is sub-optimal because it is not sort-invariant\n",
    "            # however, assuming that data production is finally complete,\n",
    "            # consequences should be limited\n",
    "            if fofn_path in fofn_collisions:\n",
    "                if VERBOSE:\n",
    "                    print(f\"FOFN path collision detected: {fofn_path.name} exists!\")\n",
    "                fofn_path = fofn_path.with_suffix(f\".{read_files_hash[:8]}.fofn\")\n",
    "                fofn_name = fofn_path.name\n",
    "                \n",
    "            fofn_collisions.add(fofn_path)\n",
    "            \n",
    "            if check_is_identical(fofn_path, fofn_name, sorted(read_files)):\n",
    "                if DEBUG:\n",
    "                    print(f\"=== ID - skipping update of {fofn_path}\")\n",
    "            elif not DRY_RUN:\n",
    "                with open(fofn_path, \"w\") as dump:\n",
    "                    for rf in sorted(read_files):\n",
    "                        assert str(rf).startswith(\"project-centric\")\n",
    "                        _ = dump.write(str(rf) + \"\\n\")\n",
    "                set_file_permissions(fofn_path)\n",
    "            else:\n",
    "                if VERBOSE:\n",
    "                    print(f\">>> DRY RUN - not updating: {fofn_path}\")\n",
    "            \n",
    "            if sample_obj.ont_complete:\n",
    "                # add Hifi fofn file to fofn table only if ONT is also complete\n",
    "                remote_fofn_path = str(fofn_path).replace(str(PATH_PREFIX.local), str(PATH_PREFIX.remote))\n",
    "                fofn_files.append((sample_obj.name, read_type, remote_fofn_path))\n",
    "                \n",
    "    if sample_obj.ont_complete:\n",
    "        for (read_type, read_group), read_files in sample_obj.sample_files.items():\n",
    "            if read_type != \"ont\":\n",
    "                continue\n",
    "                \n",
    "            read_files_hash = compute_read_files_hash(read_files)\n",
    "                \n",
    "            source_dates = sample_obj.source_dates[read_group]\n",
    "            try:\n",
    "                source_date = str(int(source_dates[0]))\n",
    "            except ValueError:\n",
    "                source_date, _ = col.Counter(source_dates).most_common(1)[0]\n",
    "            \n",
    "            read_group_info = MERGED.loc[MERGED[\"file_group\"] == read_group, [\"data_source\", \"project\"]]\n",
    "            if read_group_info.empty:\n",
    "                read_group_info = set(\n",
    "                    ALL_FILES.loc[ALL_FILES[\"file_group\"] == read_group, \"curated_by\"].values\n",
    "                )\n",
    "                data_source, project = MANUALLY_CURATED_SOURCES[read_group_info]\n",
    "            else:\n",
    "                data_source = set(read_group_info[\"data_source\"].values).pop()\n",
    "                project = set(read_group_info[\"project\"].values).pop()\n",
    "\n",
    "            if data_source == project and data_source in MANUALLY_CURATED_SOURCES:\n",
    "                data_source, project = MANUALLY_CURATED_SOURCES[data_source]\n",
    "            elif data_source == project:\n",
    "                assert data_source.startswith(\"HGSVC\") and project.startswith(\"HGSVC\"), f\"{data_source} / {project}\"\n",
    "                project = \"hgsvc\"\n",
    "                data_source = data_source[5:].upper()\n",
    "            else:\n",
    "                pass\n",
    "               \n",
    "            source_path = sample_obj.get_file_group_lca_path(read_files)\n",
    "            verified_file_dir = PATH_PREFIX.local.joinpath(\n",
    "                DATA_ROOT, source_path\n",
    "            )\n",
    "            \n",
    "            # step - write verified file\n",
    "            verified_file_name = f\"{sample_obj.name}.{len(read_files)}-cells.verified\"\n",
    "            verified_file_path = verified_file_dir.joinpath(verified_file_name)\n",
    "            verified_read_files = sorted([fp.name for fp in read_files])\n",
    "            \n",
    "            if check_is_identical(verified_file_path, verified_file_name, verified_read_files):\n",
    "                if DEBUG:\n",
    "                    print(f\"=== ID - skipping update of {verified_file_path}\")\n",
    "            elif not DRY_RUN:\n",
    "                assert verified_file_dir.is_dir()\n",
    "                with open(verified_file_path, \"w\") as dump:\n",
    "                    _ = dump.write(f\"# {TODAY}\\n\")\n",
    "                    _ = dump.write(\"\\n\".join(verified_read_files) + \"\\n\")\n",
    "                set_file_permissions(verified_file_path)\n",
    "            else:\n",
    "                if VERBOSE:\n",
    "                    print(f\">>> DRY RUN - not updating: {verified_file_path}\")\n",
    "            \n",
    "            # step - write fofn file\n",
    "            fofn_name = f\"{sample_obj.name}_{read_type}_fastq.{project}-{data_source}-{source_date}.fofn\"\n",
    "            fofn_path = LOCAL_SAMPLE_ROOT.joinpath(\n",
    "                sample_obj.name, fofn_name\n",
    "            )\n",
    "            \n",
    "            # this is sub-optimal because it is not sort-invariant\n",
    "            # however, assuming that data production is finally complete,\n",
    "            # consequences should be limited\n",
    "            if fofn_path in fofn_collisions:\n",
    "                if VERBOSE:\n",
    "                    print(f\"FOFN path collision detected: {fofn_path.name} exists!\")\n",
    "                fofn_path = fofn_path.with_suffix(f\".{read_files_hash[:8]}.fofn\")\n",
    "                fofn_name = fofn_path.name\n",
    "                \n",
    "            fofn_collisions.add(fofn_path)\n",
    "            \n",
    "            if check_is_identical(fofn_path, fofn_name, sorted(read_files)):\n",
    "                if DEBUG:\n",
    "                    print(f\"=== ID - skipping update of {fofn_path}\")\n",
    "            elif not DRY_RUN:\n",
    "                with open(fofn_path, \"w\") as dump:\n",
    "                    for rf in sorted(read_files):\n",
    "                        assert str(rf).startswith(\"project-centric\")\n",
    "                        _ = dump.write(str(rf) + \"\\n\")\n",
    "                set_file_permissions(fofn_path)\n",
    "            else:\n",
    "                if VERBOSE:\n",
    "                    print(f\">>> DRY RUN - not updating: {fofn_path}\")\n",
    "            \n",
    "            # add ONT fofn file to fofn table only if Hifi is also complete\n",
    "            if sample_obj.hifi_complete:\n",
    "                remote_fofn_path = str(fofn_path).replace(str(PATH_PREFIX.local), str(PATH_PREFIX.remote))\n",
    "                fofn_files.append((sample_obj.name, read_type, remote_fofn_path))\n",
    "            \n",
    "fofn_files = pd.DataFrame.from_records(fofn_files, columns=[\"sample\", \"read_type\", \"fofn_path\"])\n",
    "fofn_files.sort_values([\"sample\", \"read_type\"], inplace=True)\n",
    "\n",
    "if not DRY_RUN:\n",
    "    with open(fofn_files_out, \"w\") as dump:\n",
    "        _ = dump.write(f\"# {TODAY}\\n\")\n",
    "        fofn_files.to_csv(dump, sep=\"\\t\", header=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
