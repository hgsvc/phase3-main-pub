{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d243470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping  NA24143_HG004_mother\n",
      "skipping  NA24149_HG003_father\n",
      "skipping  HG00733R\n",
      "skipping  HG00733red1\n",
      "skipping  HG00733red2\n",
      "skipping  MTJCCL157\n",
      "skipping  HG00733red1\n",
      "skipping  MTJSMPMIX\n",
      "skipping  MTJSMPMIX\n",
      "skipping  HG00733red2\n",
      "skipping  HG00733R\n",
      "skipping  MTJCCL157\n"
     ]
    }
   ],
   "source": [
    "%run \"../../00_project_config.ipynb\"\n",
    "%run \"../00_path_config.ipynb\"\n",
    "\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle as pck\n",
    "import requests as req\n",
    "\n",
    "# possible remotes\n",
    "prefix_paths = {\n",
    "    \"IGSR:HGSVC2:\": \"ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/HGSVC2/working\",\n",
    "    \"IGSR:HGSVC3:\": \"ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/HGSVC3/working\",\n",
    "}\n",
    "\n",
    "output_table = PROJECT_BASE.joinpath(\n",
    "    \"annotations\", \"data_freezes\", \"out-130.hgsvc3_data_sources.files.tsv\"\n",
    ")\n",
    "\n",
    "file_cache_dump = pl.Path(\".\").resolve().parent.joinpath(\n",
    "    \".cache\", \"data-source-files.pck\"\n",
    ")\n",
    "assert file_cache_dump.is_file()\n",
    "\n",
    "with open(file_cache_dump, \"rb\") as dump:\n",
    "    file_cache = pck.load(dump)\n",
    "\n",
    "online_cache_dump = pl.Path(\".\").resolve().parent.joinpath(\n",
    "    \".cache\", \"igsr-online-files.pck\"\n",
    ")\n",
    "assert online_cache_dump.is_file()\n",
    "\n",
    "if online_cache_dump.is_file():\n",
    "    with open(online_cache_dump, \"rb\") as dump:\n",
    "        online_cache = pck.load(dump)\n",
    "else:\n",
    "    online_cache = dict()\n",
    "    \n",
    "    \n",
    "def check_online_status(subfolder, filename):\n",
    "    \n",
    "    folder_ok = \"no\"\n",
    "    file_ok = \"no\"\n",
    "    root_prefix = \"unknown\"\n",
    "    \n",
    "    for prefix, path in prefix_paths.items():\n",
    "        check_folder = f\"https://{path}/{subfolder}\"\n",
    "        resp = req.head(check_folder, allow_redirects=True)\n",
    "        if resp.status_code == 200:\n",
    "            folder_ok = \"yes\"\n",
    "            root_prefix = prefix\n",
    "            check_file = f\"{check_folder}/{filename}\"\n",
    "            resp = req.head(check_file, allow_redirects=True)\n",
    "            if resp.status_code == 200:\n",
    "                file_ok = \"yes\"\n",
    "            break\n",
    "    status = {\n",
    "        \"igsr_folder_exists\": folder_ok,\n",
    "        \"igsr_file_exists\": file_ok,\n",
    "        \"root_prefix\": root_prefix\n",
    "    }\n",
    "    return status\n",
    "           \n",
    "    \n",
    "# turn cache into file to folder map\n",
    "def postprocess_file_cache(file_cache, online_cache):\n",
    "    \n",
    "    datatype = None\n",
    "    subfolder = None\n",
    "    file_map = dict()\n",
    "    for k, v in file_cache.items():\n",
    "        if \"nanopore\" in str(k):\n",
    "            datatype = \"ont\"\n",
    "            subfolder = str(k).split(\"nanopore\")[-1].strip(\"/\")\n",
    "        if \"pacbio_hifi\" in str(k):\n",
    "            datatype = \"hifi\"\n",
    "            subfolder = str(k).split(\"pacbio_hifi\")[-1].strip(\"/\")\n",
    "        if \"mother\" in subfolder or \"father\" in subfolder:\n",
    "            print(\"skipping \", subfolder)\n",
    "            continue\n",
    "        if \"GM19320\" in subfolder or \"NA19320\" in subfolder:\n",
    "            print(\"skipping \", subfolder)\n",
    "            continue\n",
    "        assert datatype is not None, k\n",
    "        for file in v:\n",
    "            file_name = file.name\n",
    "            assert file_name not in file_map\n",
    "            if \"GM19320\" in file_name or \"NA19320\" in file_name:\n",
    "                print(\"skipping \", file_name)\n",
    "                continue            \n",
    "            \n",
    "            try:\n",
    "                online_status = online_cache[file_name]\n",
    "            except KeyError:\n",
    "                online_status = check_online_status(subfolder, file_name)\n",
    "                online_cache[file_name] = online_status\n",
    "                \n",
    "            if online_status[\"root_prefix\"] != \"unknown\":\n",
    "                remote_path = online_status[\"root_prefix\"] + \"/working/\" + subfolder\n",
    "            else:\n",
    "                remote_path = \"unknown\"\n",
    "\n",
    "            file_map[file_name] = {\n",
    "                \"datatype\": datatype,\n",
    "                \"subfolder\": subfolder,\n",
    "                \"remote_path\": remote_path,\n",
    "                \"igsr_folder_exists\": online_status[\"igsr_folder_exists\"],\n",
    "                \"igsr_file_exists\": online_status[\"igsr_file_exists\"]\n",
    "            }\n",
    "    return file_map, online_cache\n",
    "        \n",
    "file_lut, online_cache = postprocess_file_cache(file_cache, online_cache)\n",
    "\n",
    "with open(online_cache_dump, \"wb\") as dump:\n",
    "    pck.dump(online_cache, dump)\n",
    "    \n",
    "stats_files = pl.Path(\n",
    "    \"/home/ebertp/work/projects/hgsvc/2023_batch_data_tables/read_stats\"\n",
    ").glob(\"*.tsv\")\n",
    "\n",
    "\n",
    "select_stats = [\n",
    "    \"total_length_grt_0bp\", \"total_num_grt_0bp\",\n",
    "    \"cov_xfold_grt_0bp_at_3Gbp\", \"length_N50_grt_0bp\",\n",
    "    \"length_auN_grt_0bp\"\n",
    "]\n",
    "\n",
    "add_ont = [\n",
    "    \"total_num_grt_100kbp\", \"cov_xfold_grt_100kbp_at_3Gbp\",\n",
    "    \"length_N50_grt_100kbp\", \"length_auN_grt_100kbp\",\n",
    "    \"total_num_grt_1Mbp\"\n",
    "]\n",
    "\n",
    "def read_stats_minimal_info(file_path, file_lut):\n",
    "    \n",
    "    sample = file_path.name.split(\"_\")[0]\n",
    "    datatype = file_path.name.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    keep_stats = select_stats\n",
    "    if datatype == \"ont\":\n",
    "        keep_stats.extend(add_ont)\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", header=0)\n",
    "    df = df.loc[df[\"statistic\"].isin(keep_stats), :].copy()\n",
    "    df.rename({\"source\": \"filename\"}, axis=1, inplace=True)\n",
    "    df[\"SIN\"] = f\"SIN:{sample[2:]}\"\n",
    "    df[\"datatype\"] = datatype\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = df.pivot(\n",
    "        index=[\"SIN\", \"filename\", \"datatype\"],\n",
    "        columns=[\"statistic\"]\n",
    "    )\n",
    "    df.columns = df.columns.droplevel(0)\n",
    "    df = df.reset_index(drop=False, inplace=False)\n",
    "        \n",
    "    df[\"subfolder\"] = df[\"filename\"].apply(\n",
    "        lambda x: file_lut.get(x, {\"subfolder\": \"unknown\"})[\"subfolder\"]\n",
    "    )\n",
    "    df[\"remote_path\"] = df[\"filename\"].apply(\n",
    "        lambda x: file_lut.get(x, {\"remote_path\": \"unknown\"})[\"remote_path\"]\n",
    "    )\n",
    "    df[\"igsr_folder_exists\"] = df[\"filename\"].apply(\n",
    "        lambda x: file_lut.get(x, {\"igsr_folder_exists\": \"no\"})[\"igsr_folder_exists\"]\n",
    "    )\n",
    "    df[\"igsr_file_exists\"] = df[\"filename\"].apply(\n",
    "        lambda x: file_lut.get(x, {\"igsr_file_exists\": \"no\"})[\"igsr_file_exists\"]\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "all_file_stats = []\n",
    "for stats_file in stats_files:\n",
    "    sample = stats_file.name.split(\"_\")[0]\n",
    "    if len(sample) > 7 or sample in [\"GM19320\", \"NA19320\"]:\n",
    "        print(\"skipping \", sample)\n",
    "        continue\n",
    "    data_stats = read_stats_minimal_info(stats_file, file_lut)\n",
    "    all_file_stats.append(data_stats)\n",
    "    \n",
    "all_file_stats = pd.concat(all_file_stats, axis=0, ignore_index=False)\n",
    "all_file_stats.fillna(0, inplace=True)\n",
    "all_file_stats.sort_values([\"SIN\", \"filename\"], inplace=True)\n",
    "all_file_stats.set_index(\"SIN\", inplace=True)\n",
    "\n",
    "#### following: explicit sanity checks\n",
    "# no file that is on the file system (known) can be unused [explicit exceptions below]\n",
    "# all files that were used must be known (be on the file system)\n",
    "\n",
    "all_used_files = set(all_file_stats[\"filename\"].values) - set([\"all\"])\n",
    "\n",
    "all_known_files = set(file_lut.keys())\n",
    "\n",
    "unused_files = all_known_files - all_used_files\n",
    "missed_files = all_used_files - all_known_files\n",
    "assert len(missed_files) == 0, sorted(missed_files)\n",
    "\n",
    "revio_733 = [\n",
    "    \"m84039_230304_003049_s1.hifi_reads.bc2039.fastq.gz\",\n",
    "    \"m84039_230307_013359_s4.hifi_reads.bc2039.fastq.gz\",\n",
    "    \"m84039_230308_011313_s3.hifi_reads.bc2039.fastq.gz\",\n",
    "    \"m84046_230324_222350_s1.hifi_reads.bc2039.fastq.gz\"\n",
    "]\n",
    "\n",
    "epi_hg002 = [\n",
    "    \"NA24385_20190125_UL_ext-prom1-1-E5-H5-PAD29338_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz\",\n",
    "    \"NA24385_20190127_UL_ext-prom1-1-A9-D9-PAD28937_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz\",\n",
    "    \"NA24385_20190128_UL_ext-prom1-1-A9-D9-PAD28937_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz\",\n",
    "    \"NA24385_20190128_UL_ext-prom1-1-E9-H9-PAD28926_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz\",\n",
    "    \"NA24385_20190129_UL_ext-prom1-1-A9-D9-PAD28937_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz\",\n",
    "    \"NA24385_20190129_UL_ext-prom1-1-E9-H9-PAD28926_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz\"\n",
    "]\n",
    "\n",
    "save_to_ignore = 0\n",
    "ignored = []\n",
    "for unf in unused_files:\n",
    "    file_info = file_lut[unf]\n",
    "    if \"mother\" in file_info[\"subfolder\"] or \"father\" in file_info[\"subfolder\"]:\n",
    "        # filtered above now\n",
    "        save_to_ignore += 1\n",
    "        ignored.append(unf)\n",
    "        continue\n",
    "    if unf in revio_733:\n",
    "        save_to_ignore += 1\n",
    "        ignored.append(unf)\n",
    "        continue\n",
    "    if unf in epi_hg002:\n",
    "        save_to_ignore += 1\n",
    "        ignored.append(unf)\n",
    "        continue\n",
    "    print(unf)\n",
    "\n",
    "# last assertion of sanity checks\n",
    "assert len(all_used_files) + save_to_ignore == len(all_known_files)  \n",
    "\n",
    "# norm column data types\n",
    "for c in all_file_stats.columns:\n",
    "    if \"length\" in c or \"num\" in c:\n",
    "        all_file_stats[c] = all_file_stats[c].astype(int)\n",
    "\n",
    "with open(output_table, \"w\") as dump:\n",
    "    _ = dump.write(f\"# {TODAY}\\n\")\n",
    "    all_file_stats.to_csv(dump, sep=\"\\t\", header=True, index=True, index_label=\"SIN\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
