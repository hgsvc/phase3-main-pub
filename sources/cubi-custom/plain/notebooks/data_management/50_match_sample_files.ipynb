{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e2a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping over 19320 - not enough data\n",
      "\n",
      "Skipping over 19320 - not enough data\n"
     ]
    }
   ],
   "source": [
    "%run \"../00_project_config.ipynb\"\n",
    "%run \"05_annotations.ipynb\"\n",
    "%run \"10_data_objects.ipynb\"\n",
    "%run \"20_process_sample_table.ipynb\"\n",
    "%run \"30_process_data_sources.ipynb\"\n",
    "\n",
    "MANUALLY_CURATED_FOLDERS = {\n",
    "    \"20230501_HGSVC_UL_ONT-UW\": \"UW_WH\",\n",
    "    \"20211013_ONT_Rebasecalled\": \"JAX_PA\",\n",
    "    \"20230706_HGSVC_EEE_UL_ONT\": \"UW_WH\",\n",
    "    \"20230703_HGSVC_EEE_HIFI\": \"UW_WH\",\n",
    "    \"20220831_JAX_HiFi\": \"JAX_PA\",\n",
    "    \"20230905_HGSVC_EEE_UL_ONT\": \"UW_WH\",\n",
    "    \"20231126_UW_HiFi\": \"UW_WH\",\n",
    "    \"20240117_UW_HiFi\": \"UW_WH\"\n",
    "}\n",
    "\n",
    "# Added for cases where an additional cell was\n",
    "# sequenced to replace a contaminated one\n",
    "# --- NA18939\n",
    "RESEQ_REPLACEMENTS = {\n",
    "    \"m64076_230717_214340-bc2082\": \"UW_WH\",\n",
    "}\n",
    "\n",
    "AUTO_TABLE_ALL_KNOWN = PROJECT_BASE.joinpath(\"annotations\", \"autogen\", \"annotated_files.tsv\")\n",
    "AUTO_TABLE_ALL_KNOWN.parent.mkdir(parents=True, exist_ok=True)\n",
    "AUTO_TABLE_ALL_FILES = PROJECT_BASE.joinpath(\"annotations\", \"autogen\", \"existing_files.tsv\")\n",
    "AUTO_TABLE_ALL_FILES.parent.mkdir(parents=True, exist_ok=True)\n",
    "AUTO_TABLE_ERR_FILES = PROJECT_BASE.joinpath(\"annotations\", \"autogen\", \"error_files.tsv\")\n",
    "AUTO_TABLE_ERR_FILES.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "matched_records = []\n",
    "# iterate through all files annotated in a metadata\n",
    "# table (that largely applies to only batch 1)\n",
    "for row in KNOWN_FILES.itertuples(index=True):\n",
    "    # for each annotated file (row), check if it\n",
    "    # matches with existing files of the respective sample\n",
    "    files_for_sample = FILES_EXIST_PER_SAMPLE[row.sample]\n",
    "    \n",
    "    matched_files = []\n",
    "    num_matches = 0\n",
    "    sample_batch = SAMPLES[row.sample].batch_num\n",
    "    \n",
    "    for file in files_for_sample:\n",
    "        if row.read_type != file.read_type:\n",
    "            continue\n",
    "        if row.cell in file.file_name:\n",
    "            # perfect / metadata match\n",
    "            assert row.read_type == file.read_type\n",
    "            matched_records.append(\n",
    "                (\n",
    "                    row.Index, sample_batch,\n",
    "                    file.data_rel_path, file.file_date,\n",
    "                    file.file_group, file.group_date, 1\n",
    "                )\n",
    "            )\n",
    "            matched_files.append(file)\n",
    "            num_matches += 1\n",
    "    if num_matches > 1:\n",
    "        # error\n",
    "        for mf in matched_files:\n",
    "            print(mf)\n",
    "        raise ValueError(f\"Multi-match: {row}\")\n",
    "        \n",
    "    if num_matches == 1:\n",
    "        # change the underlying data file object\n",
    "        matched_files[0].set_matched_entry(row.Index)\n",
    "        matched_files[0].set_curator(\"metadata_table\")\n",
    "    \n",
    "matched_records = pd.DataFrame.from_records(\n",
    "    matched_records,\n",
    "    columns=[\"index\", \"processing_batch\", \"file_rel_path\", \"file_date\", \"file_group\", \"group_date\", \"matched\"]\n",
    ")\n",
    "matched_records.index = matched_records[\"index\"]\n",
    "matched_records.drop(\"index\", inplace=True, axis=1)\n",
    "\n",
    "MERGED = KNOWN_FILES.merge(matched_records, how=\"outer\", left_index=True, right_index=True)\n",
    "MERGED[\"matched\"].fillna(0, inplace=True)\n",
    "MERGED[\"processing_batch\"].fillna(-1, inplace=True)\n",
    "MERGED.fillna(\"n/a\", inplace=True)\n",
    "MERGED[\"matched\"] = MERGED[\"matched\"].astype(int)\n",
    "MERGED[\"processing_batch\"] = MERGED[\"processing_batch\"].astype(int)\n",
    "MERGED[\"file_name\"] = MERGED[\"file_rel_path\"].apply(lambda x: \"n/a\" if x == \"n/a\" else pl.Path(x).name)\n",
    "MERGED.sort_index(inplace=True)\n",
    "\n",
    "curated_records = []\n",
    "unknown_records = []\n",
    "# iterate through all files per sample\n",
    "for sample, files_for_sample in FILES_EXIST_PER_SAMPLE.items():\n",
    "    sample_batch = SAMPLES[sample].batch_num\n",
    "    for sample_file in files_for_sample:\n",
    "        if sample_file.matched_entry is None:\n",
    "            # check if manually curated\n",
    "            for folder, curator in MANUALLY_CURATED_FOLDERS.items():\n",
    "                if folder in str(sample_file.data_rel_path):\n",
    "                    sample_file.set_matched_entry(-1)\n",
    "                    sample_file.set_curator(curator)\n",
    "                    sample_file.set_sample_batch(sample_batch)\n",
    "                    break\n",
    "\n",
    "            # check if new seq. cell\n",
    "            if sample_file.matched_entry is None:\n",
    "                for cell_id, curator in RESEQ_REPLACEMENTS.items():\n",
    "                    if cell_id in str(sample_file.data_rel_path):\n",
    "                        sample_file.set_matched_entry(-1)\n",
    "                        sample_file.set_curator(curator)\n",
    "                        sample_file.set_sample_batch(sample_batch)\n",
    "                        break\n",
    "\n",
    "            if sample_file.matched_entry is None:\n",
    "                sample_file.set_curator(\"error\")\n",
    "                sample_file.set_sample_batch(sample_batch)\n",
    "                unknown_records.append(sample_file.get_table_row(batch_num=True))\n",
    "            else:\n",
    "                curated_records.append(sample_file.get_table_row(batch_num=True))\n",
    "\n",
    "ERR_FILES = pd.DataFrame.from_records(\n",
    "    unknown_records, columns=sample_file.get_table_header(batch_num=True)\n",
    ")                \n",
    "\n",
    "curated_records = pd.DataFrame.from_records(\n",
    "    curated_records, columns=sample_file.get_table_header(batch_num=True))\n",
    "curated_records[\"alt_id\"] = curated_records[\"file_name\"]\n",
    "curated_records[\"file_rel_path\"] = curated_records[\"file_path\"]\n",
    "curated_records[\"cell\"] = \"cell-id-na\"\n",
    "curated_records[\"data_source\"] = curated_records[\"curated_by\"]\n",
    "curated_records[\"file_date\"] = \"unset\"\n",
    "curated_records[\"file_source\"] = \"manual\"\n",
    "curated_records[\"project\"] = curated_records[\"curated_by\"]\n",
    "\n",
    "curated_records.drop(\"file_path\", axis=1, inplace=True)\n",
    "\n",
    "MERGED[\"curated_by\"] = \"metadata\"\n",
    "\n",
    "MERGED = pd.concat([MERGED, curated_records], axis=0, ignore_index=False)\n",
    "\n",
    "remove_no_file = MERGED[\"file_name\"] == \"n/a\"\n",
    "if sum(remove_no_file) > 0:\n",
    "    ERR_FILES = pd.concat([ERR_FILES, MERGED.loc[remove_no_file, :].copy()], axis=0, ignore_index=False)\n",
    "    MERGED = MERGED.loc[~remove_no_file, :].copy()\n",
    "\n",
    "MERGED.sort_values([\"sample\", \"read_type\", \"file_rel_path\"], inplace=True)\n",
    "\n",
    "MERGED.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# files annotated or in a curated folder\n",
    "with open(AUTO_TABLE_ALL_KNOWN, \"w\") as dump_table:\n",
    "    _ = dump_table.write(f\"# {TODAY}\\n\")\n",
    "    MERGED.to_csv(\n",
    "        dump_table,\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        index=True,\n",
    "        index_label=\"index\"\n",
    "    )\n",
    "\n",
    "    \n",
    "# # files that exist in a file system location\n",
    "# # with a sample listing file\n",
    "# ALL_FILES = pd.DataFrame.from_records(\n",
    "#     all_files_table,\n",
    "#     columns=all_files_header\n",
    "# )\n",
    "# ALL_FILES.sort_values([\"sample\", \"read_type\", \"file_group\", \"file_name\"], inplace=True)\n",
    "# with open(AUTO_TABLE_ALL_FILES, \"w\") as dump_table:\n",
    "#     _ = dump_table.write(f\"# {TODAY}\\n\")\n",
    "#     ALL_FILES.to_csv(\n",
    "#         dump_table,\n",
    "#         sep=\"\\t\",\n",
    "#         header=True,\n",
    "#         index=False\n",
    "#     )\n",
    "\n",
    "# unknown files\n",
    "with open(AUTO_TABLE_ERR_FILES, \"w\") as dump_table:\n",
    "    _ = dump_table.write(f\"# {TODAY}\\n\")\n",
    "    ERR_FILES.to_csv(\n",
    "        dump_table,\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
